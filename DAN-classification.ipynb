{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a4807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:29:44.832684Z",
     "iopub.status.busy": "2024-07-11T02:29:44.832240Z",
     "iopub.status.idle": "2024-07-11T02:29:58.964046Z",
     "shell.execute_reply": "2024-07-11T02:29:58.962785Z"
    },
    "id": "yFlgbzpc2BCl",
    "outputId": "8d0e1e0f-2e2e-41a2-b393-da64ff64c0d5",
    "papermill": {
     "duration": 14.155007,
     "end_time": "2024-07-11T02:29:58.966828",
     "exception": false,
     "start_time": "2024-07-11T02:29:44.811821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9940ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:29:59.006094Z",
     "iopub.status.busy": "2024-07-11T02:29:59.005190Z",
     "iopub.status.idle": "2024-07-11T02:30:05.786087Z",
     "shell.execute_reply": "2024-07-11T02:30:05.785136Z"
    },
    "id": "hgDCyl06yP7A",
    "papermill": {
     "duration": 6.802983,
     "end_time": "2024-07-11T02:30:05.788749",
     "exception": false,
     "start_time": "2024-07-11T02:29:58.985766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from scipy.stats import chi2\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e28c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:05.829211Z",
     "iopub.status.busy": "2024-07-11T02:30:05.828151Z",
     "iopub.status.idle": "2024-07-11T02:30:05.919982Z",
     "shell.execute_reply": "2024-07-11T02:30:05.919034Z"
    },
    "id": "uOlc2RS6yP7F",
    "outputId": "08cb8289-c133-4eb9-bcbe-233068e7f22f",
    "papermill": {
     "duration": 0.114528,
     "end_time": "2024-07-11T02:30:05.922243",
     "exception": false,
     "start_time": "2024-07-11T02:30:05.807715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22423bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:05.961971Z",
     "iopub.status.busy": "2024-07-11T02:30:05.961173Z",
     "iopub.status.idle": "2024-07-11T02:30:05.967007Z",
     "shell.execute_reply": "2024-07-11T02:30:05.966075Z"
    },
    "id": "qh0JctlXyP7G",
    "papermill": {
     "duration": 0.027789,
     "end_time": "2024-07-11T02:30:05.969201",
     "exception": false,
     "start_time": "2024-07-11T02:30:05.941412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configuration for training, you should modify these values to get the best performance\n",
    "config = {\n",
    "    \"num_labels\": 6,\n",
    "    \"hidden_dropout_prob\": 0.15,\n",
    "    \"hidden_size\": 768,\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "training_parameters = {\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 15,\n",
    "    \"output_folder\": \"/kaggle/working\",\n",
    "    \"output_file\": \"model.bin\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"print_after_steps\": 100,\n",
    "    \"save_steps\": 5000,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84cb36",
   "metadata": {
    "id": "Ovg1yCTJyP7G",
    "papermill": {
     "duration": 0.018408,
     "end_time": "2024-07-11T02:30:06.006965",
     "exception": false,
     "start_time": "2024-07-11T02:30:05.988557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class for preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6de20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.046911Z",
     "iopub.status.busy": "2024-07-11T02:30:06.046161Z",
     "iopub.status.idle": "2024-07-11T02:30:06.057425Z",
     "shell.execute_reply": "2024-07-11T02:30:06.056594Z"
    },
    "id": "IeKwKDl4yP7I",
    "papermill": {
     "duration": 0.033928,
     "end_time": "2024-07-11T02:30:06.059797",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.025869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.iloc[index][\"text\"]\n",
    "        attack = self.df.iloc[index][\"label\"]\n",
    "        attack_dict = {'Injection': 0,\n",
    "          'Manipulation': 1,\n",
    "          'Scanning for Vulnerable Software': 2,\n",
    "          'HTTP abusion': 3,\n",
    "          'Fake the Source of Data': 4,\n",
    "                      'Normal': 5}\n",
    "        label = attack_dict[attack]\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "                review,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 512,\n",
    "                padding=\"max_length\",\n",
    "                return_overflowing_tokens=True,\n",
    "                truncation = True,\n",
    "            )\n",
    "        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n",
    "            # print(\"Attention! you are cropping tokens\")\n",
    "            pass\n",
    "\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n",
    "\n",
    "        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n",
    "\n",
    "\n",
    "\n",
    "        data_input = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }\n",
    "\n",
    "        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36256d",
   "metadata": {
    "id": "GgGrAD-3yP7K",
    "papermill": {
     "duration": 0.018757,
     "end_time": "2024-07-11T02:30:06.097804",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.079047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class for MMD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c41e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.137793Z",
     "iopub.status.busy": "2024-07-11T02:30:06.136991Z",
     "iopub.status.idle": "2024-07-11T02:30:06.163780Z",
     "shell.execute_reply": "2024-07-11T02:30:06.162675Z"
    },
    "id": "8TrDpuXuyP7L",
    "papermill": {
     "duration": 0.049632,
     "end_time": "2024-07-11T02:30:06.166335",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.116703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "class GaussianKernel(nn.Module):\n",
    "    r\"\"\"Gaussian Kernel Matrix\n",
    "    Gaussian Kernel k is defined by\n",
    "    .. math::\n",
    "        k(x_1, x_2) = \\exp \\left( - \\dfrac{\\| x_1 - x_2 \\|^2}{2\\sigma^2} \\right)\n",
    "    where :math:`x_1, x_2 \\in R^d` are 1-d tensors.\n",
    "    Gaussian Kernel Matrix K is defined on input group :math:`X=(x_1, x_2, ..., x_m),`\n",
    "    .. math::\n",
    "        K(X)_{i,j} = k(x_i, x_j)\n",
    "    Also by default, during training this layer keeps running estimates of the\n",
    "    mean of L2 distances, which are then used to set hyperparameter  :math:`\\sigma`.\n",
    "    Mathematically, the estimation is :math:`\\sigma^2 = \\dfrac{\\alpha}{n^2}\\sum_{i,j} \\| x_i - x_j \\|^2`.\n",
    "    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
    "    keep running estimates, and use a fixed :math:`\\sigma` instead.\n",
    "    Args:\n",
    "        sigma (float, optional): bandwidth :math:`\\sigma`. Default: None\n",
    "        track_running_stats (bool, optional): If ``True``, this module tracks the running mean of :math:`\\sigma^2`.\n",
    "          Otherwise, it won't track such statistics and always uses fix :math:`\\sigma^2`. Default: ``True``\n",
    "        alpha (float, optional): :math:`\\alpha` which decides the magnitude of :math:`\\sigma^2` when track_running_stats is set to ``True``\n",
    "    Inputs:\n",
    "        - X (tensor): input group :math:`X`\n",
    "    Shape:\n",
    "        - Inputs: :math:`(minibatch, F)` where F means the dimension of input features.\n",
    "        - Outputs: :math:`(minibatch, minibatch)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Optional[float] = None, track_running_stats: Optional[bool] = True,\n",
    "                 alpha: Optional[float] = 1.):\n",
    "        super(GaussianKernel, self).__init__()\n",
    "        assert track_running_stats or sigma is not None\n",
    "        self.sigma_square = torch.tensor(sigma * sigma) if sigma is not None else None\n",
    "        self.track_running_stats = track_running_stats\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        l2_distance_square = ((X.unsqueeze(0) - X.unsqueeze(1)) ** 2).sum(2)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.sigma_square = self.alpha * torch.mean(l2_distance_square.detach())\n",
    "\n",
    "        return torch.exp(-l2_distance_square / (2 * self.sigma_square))\n",
    "\n",
    "class MultipleKernelMaximumMeanDiscrepancy(nn.Module):\n",
    "    r\"\"\"The Multiple Kernel Maximum Mean Discrepancy (MK-MMD) used in\n",
    "    `Learning Transferable Features with Deep Adaptation Networks (ICML 2015) <https://arxiv.org/pdf/1502.02791>`_\n",
    "    Given source domain :math:`\\mathcal{D}_s` of :math:`n_s` labeled points and target domain :math:`\\mathcal{D}_t`\n",
    "    of :math:`n_t` unlabeled points drawn i.i.d. from P and Q respectively, the deep networks will generate\n",
    "    activations as :math:`\\{z_i^s\\}_{i=1}^{n_s}` and :math:`\\{z_i^t\\}_{i=1}^{n_t}`.\n",
    "    The MK-MMD :math:`D_k (P, Q)` between probability distributions P and Q is defined as\n",
    "    .. math::\n",
    "        D_k(P, Q) \\triangleq \\| E_p [\\phi(z^s)] - E_q [\\phi(z^t)] \\|^2_{\\mathcal{H}_k},\n",
    "    :math:`k` is a kernel function in the function space\n",
    "    .. math::\n",
    "        \\mathcal{K} \\triangleq \\{ k=\\sum_{u=1}^{m}\\beta_{u} k_{u} \\}\n",
    "    where :math:`k_{u}` is a single kernel.\n",
    "    Using kernel trick, MK-MMD can be computed as\n",
    "    .. math::\n",
    "        \\hat{D}_k(P, Q) &=\n",
    "        \\dfrac{1}{n_s^2} \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_s} k(z_i^{s}, z_j^{s})\\\\\n",
    "        &+ \\dfrac{1}{n_t^2} \\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t} k(z_i^{t}, z_j^{t})\\\\\n",
    "        &- \\dfrac{2}{n_s n_t} \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_t} k(z_i^{s}, z_j^{t}).\\\\\n",
    "    Args:\n",
    "        kernels (tuple(torch.nn.Module)): kernel functions.\n",
    "        linear (bool): whether use the linear version of DAN. Default: False\n",
    "    Inputs:\n",
    "        - z_s (tensor): activations from the source domain, :math:`z^s`\n",
    "        - z_t (tensor): activations from the target domain, :math:`z^t`\n",
    "    Shape:\n",
    "        - Inputs: :math:`(minibatch, *)`  where * means any dimension\n",
    "        - Outputs: scalar\n",
    "    .. note::\n",
    "        Activations :math:`z^{s}` and :math:`z^{t}` must have the same shape.\n",
    "    .. note::\n",
    "        The kernel values will add up when there are multiple kernels.\n",
    "    Examples::\n",
    "        >>> from tllib.modules.kernels import GaussianKernel\n",
    "        >>> feature_dim = 1024\n",
    "        >>> batch_size = 10\n",
    "        >>> kernels = (GaussianKernel(alpha=0.5), GaussianKernel(alpha=1.), GaussianKernel(alpha=2.))\n",
    "        >>> loss = MultipleKernelMaximumMeanDiscrepancy(kernels)\n",
    "        >>> # features from source domain and target domain\n",
    "        >>> z_s, z_t = torch.randn(batch_size, feature_dim), torch.randn(batch_size, feature_dim)\n",
    "        >>> output = loss(z_s, z_t)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernels: Sequence[nn.Module], linear: Optional[bool] = False):\n",
    "        super(MultipleKernelMaximumMeanDiscrepancy, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.index_matrix = None\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, z_s: torch.Tensor, z_t: torch.Tensor) -> torch.Tensor:\n",
    "        features = torch.cat([z_s, z_t], dim=0)\n",
    "        batch_size = int(z_s.size(0))\n",
    "        self.index_matrix = _update_index_matrix(batch_size, self.index_matrix, self.linear).to(z_s.device)\n",
    "\n",
    "\n",
    "        kernel_matrix = sum([kernel(features) for kernel in self.kernels])  # Add up the matrix of each kernel\n",
    "        # Add 2 / (n-1) to make up for the value on the diagonal\n",
    "        # to ensure loss is positive in the non-linear version\n",
    "        loss = (kernel_matrix * self.index_matrix).sum() + 2. / float(batch_size - 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _update_index_matrix(batch_size: int, index_matrix: Optional[torch.Tensor] = None,\n",
    "                         linear: Optional[bool] = True) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Update the `index_matrix` which convert `kernel_matrix` to loss.\n",
    "    If `index_matrix` is a tensor with shape (2 x batch_size, 2 x batch_size), then return `index_matrix`.\n",
    "    Else return a new tensor with shape (2 x batch_size, 2 x batch_size).\n",
    "    \"\"\"\n",
    "    if index_matrix is None or index_matrix.size(0) != batch_size * 2:\n",
    "        index_matrix = torch.zeros(2 * batch_size, 2 * batch_size)\n",
    "        if linear:\n",
    "            for i in range(batch_size):\n",
    "                s1, s2 = i, (i + 1) % batch_size\n",
    "                t1, t2 = s1 + batch_size, s2 + batch_size\n",
    "                index_matrix[s1, s2] = 1. / float(batch_size)\n",
    "                index_matrix[t1, t2] = 1. / float(batch_size)\n",
    "                index_matrix[s1, t2] = -1. / float(batch_size)\n",
    "                index_matrix[s2, t1] = -1. / float(batch_size)\n",
    "        else:\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    if i != j:\n",
    "                        index_matrix[i][j] = 1. / float(batch_size * (batch_size - 1))\n",
    "                        index_matrix[i + batch_size][j + batch_size] = 1. / float(batch_size * (batch_size - 1))\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    index_matrix[i][j + batch_size] = -1. / float(batch_size * batch_size)\n",
    "                    index_matrix[i + batch_size][j] = -1. / float(batch_size * batch_size)\n",
    "    return index_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a1f24",
   "metadata": {
    "papermill": {
     "duration": 0.018951,
     "end_time": "2024-07-11T02:30:06.205721",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.186770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a29ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.245594Z",
     "iopub.status.busy": "2024-07-11T02:30:06.245198Z",
     "iopub.status.idle": "2024-07-11T02:30:06.255677Z",
     "shell.execute_reply": "2024-07-11T02:30:06.254229Z"
    },
    "papermill": {
     "duration": 0.032909,
     "end_time": "2024-07-11T02:30:06.257847",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.224938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cmd(src_embed, tgt_embed, n_moments):\n",
    "    if torch.mean(torch.abs(src_embed) + torch.abs(tgt_embed)) <= 1e-7:\n",
    "        print(\"Warning: feature representations tend towards zero. \"\n",
    "              \"Consider decreasing 'da_lambda' or using lambda schedule.\")\n",
    "\n",
    "    src_mean = src_embed.mean(dim=0)\n",
    "    tgt_mean = tgt_embed.mean(dim=0)\n",
    "\n",
    "    src_centered = src_embed - src_mean\n",
    "    tgt_centered = tgt_embed - tgt_mean\n",
    "\n",
    "    first_moment = l2diff(src_mean, tgt_mean)  # start with first moment\n",
    "\n",
    "    moments_diff_sum = first_moment\n",
    "    for k in range(2, n_moments + 1):\n",
    "        moments_diff_sum = moments_diff_sum + moment_diff(src_centered, tgt_centered, k)\n",
    "\n",
    "    return moments_diff_sum\n",
    "\n",
    "\n",
    "def l2diff(src, tgt):\n",
    "    \"\"\"\n",
    "    standard euclidean norm. small number added to increase numerical stability.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.sum((src - tgt) ** 2) + 1e-8)\n",
    "\n",
    "\n",
    "def moment_diff(src, tgt, moment):\n",
    "    \"\"\"\n",
    "    difference between moments\n",
    "    \"\"\"\n",
    "    ss1 = (src ** moment).mean(0)\n",
    "    ss2 = (tgt ** moment).mean(0)\n",
    "    return l2diff(ss1, ss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01454217",
   "metadata": {
    "papermill": {
     "duration": 0.018651,
     "end_time": "2024-07-11T02:30:06.295685",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.277034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SWD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c76a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.335414Z",
     "iopub.status.busy": "2024-07-11T02:30:06.334715Z",
     "iopub.status.idle": "2024-07-11T02:30:06.345064Z",
     "shell.execute_reply": "2024-07-11T02:30:06.344003Z"
    },
    "papermill": {
     "duration": 0.032993,
     "end_time": "2024-07-11T02:30:06.347388",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.314395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def swd(src_embed, tgt_embed, multiplier, p):\n",
    "    projections = torch.zeros((src_embed.size(1), src_embed.size(1) * multiplier),\n",
    "                              device=tgt_embed.device).normal_(0, 1)\n",
    "    projections = projections / torch.norm(projections, p=p, dim=0, keepdim=True)\n",
    "\n",
    "    # repeat target batch size to be the same size as source\n",
    "    src_batch_size = src_embed.size(0)\n",
    "    tgt_batch_size = tgt_embed.size(0)\n",
    "    batch_size = src_batch_size + tgt_batch_size\n",
    "    src_repeats = math.ceil(batch_size / src_batch_size)\n",
    "    tgt_repeats = math.ceil(batch_size / tgt_batch_size)\n",
    "    src_embed_rep = torch.cat([src_embed] * src_repeats, dim=0)[:batch_size]\n",
    "    tgt_embed_rep = torch.cat([tgt_embed] * tgt_repeats, dim=0)[:batch_size]\n",
    "\n",
    "    # project both samples 'num_projections' times\n",
    "    pr_src = src_embed_rep.mm(projections)\n",
    "    pr_tgt = tgt_embed_rep.mm(projections)\n",
    "\n",
    "    # sort the projection results\n",
    "    pr_sim = torch.sort(pr_src, dim=0)[0]\n",
    "    pr_meas = torch.sort(pr_tgt, dim=0)[0]\n",
    "    sliced_wd = torch.pow(pr_sim - pr_meas, p)\n",
    "\n",
    "    # return mean distance, scaled by batch size\n",
    "    return sliced_wd.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233a75e",
   "metadata": {
    "papermill": {
     "duration": 0.018463,
     "end_time": "2024-07-11T02:30:06.384619",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.366156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CoRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec972156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.424485Z",
     "iopub.status.busy": "2024-07-11T02:30:06.423625Z",
     "iopub.status.idle": "2024-07-11T02:30:06.432225Z",
     "shell.execute_reply": "2024-07-11T02:30:06.431214Z"
    },
    "papermill": {
     "duration": 0.030888,
     "end_time": "2024-07-11T02:30:06.434334",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.403446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def comp_cov(x):\n",
    "    xm = x - torch.mean(x, dim=0, keepdim=True)\n",
    "    return xm.T @ xm / (x.size(0) - 1)\n",
    "\n",
    "\n",
    "def coral(src_embed, tgt_embed):\n",
    "    src_batch_size = src_embed.size(0)\n",
    "    tgt_batch_size = tgt_embed.size(0)\n",
    "    batch_size = src_batch_size + tgt_batch_size\n",
    "    src_repeats = math.ceil(batch_size / src_batch_size)\n",
    "    tgt_repeats = math.ceil(batch_size / tgt_batch_size)\n",
    "    # handle case when source and target are not of same size\n",
    "    src_embed_rep = torch.cat([src_embed] * src_repeats, dim=0)[:batch_size]\n",
    "    tgt_embed_rep = torch.cat([tgt_embed] * tgt_repeats, dim=0)[:batch_size]\n",
    "    d = src_embed_rep.size()[1]\n",
    "    src_cov = comp_cov(src_embed_rep)\n",
    "    tgt_cov = comp_cov(tgt_embed_rep)\n",
    "\n",
    "    # squared matrix frobenius norm\n",
    "    loss = torch.sum((src_cov - tgt_cov)**2)\n",
    "    loss = loss / (4 * d * d)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a305a",
   "metadata": {
    "id": "xIGv34RQyP7O",
    "papermill": {
     "duration": 0.0185,
     "end_time": "2024-07-11T02:30:06.471699",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.453199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import dataset include source dataset and target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852459a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:06.510453Z",
     "iopub.status.busy": "2024-07-11T02:30:06.509503Z",
     "iopub.status.idle": "2024-07-11T02:30:12.269555Z",
     "shell.execute_reply": "2024-07-11T02:30:12.268574Z"
    },
    "id": "fODJjJSSyP7O",
    "outputId": "55fc4b46-ab40-440d-f784-808f289515a0",
    "papermill": {
     "duration": 5.781968,
     "end_time": "2024-07-11T02:30:12.271856",
     "exception": false,
     "start_time": "2024-07-11T02:30:06.489888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/srbh2020-v2/dataset_capec_combine (1).csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dc7cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:12.310439Z",
     "iopub.status.busy": "2024-07-11T02:30:12.310080Z",
     "iopub.status.idle": "2024-07-11T02:30:12.865053Z",
     "shell.execute_reply": "2024-07-11T02:30:12.864126Z"
    },
    "id": "yHJtaKIryP7P",
    "outputId": "74a7d6b9-b0e4-40ca-de3d-f4cedb6494ee",
    "papermill": {
     "duration": 0.577288,
     "end_time": "2024-07-11T02:30:12.867718",
     "exception": false,
     "start_time": "2024-07-11T02:30:12.290430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional (not effect very much)\n",
    "# for word tokenizer instead of character tokenizer\n",
    "\n",
    "df_train['label'] = df_train['category']\n",
    "df_train = df_train.sample(frac = 1)\n",
    "\n",
    "df_nor = df_train[(df_train['label'] == 'Normal')].sample(30000)\n",
    "df_train = df_train[(df_train['label'] != 'Normal')]\n",
    "\n",
    "df_train = pd.concat([df_nor, df_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f920f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:12.906456Z",
     "iopub.status.busy": "2024-07-11T02:30:12.906082Z",
     "iopub.status.idle": "2024-07-11T02:30:13.835918Z",
     "shell.execute_reply": "2024-07-11T02:30:13.834686Z"
    },
    "id": "uLnwXXoXyP7P",
    "outputId": "bc7e7c06-1e41-4571-e924-91e84409f9db",
    "papermill": {
     "duration": 0.952032,
     "end_time": "2024-07-11T02:30:13.838657",
     "exception": false,
     "start_time": "2024-07-11T02:30:12.886625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## prepare for training\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train['text'], df_train['label'],test_size=0.3, stratify=df_train['label'], shuffle = True)\n",
    "df_train = pd.concat([X_train, Y_train], axis=1)\n",
    "df_test = pd.concat([X_test, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f0e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:13.878861Z",
     "iopub.status.busy": "2024-07-11T02:30:13.878436Z",
     "iopub.status.idle": "2024-07-11T02:30:13.931055Z",
     "shell.execute_reply": "2024-07-11T02:30:13.930181Z"
    },
    "papermill": {
     "duration": 0.075192,
     "end_time": "2024-07-11T02:30:13.933331",
     "exception": false,
     "start_time": "2024-07-11T02:30:13.858139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c98fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:13.972872Z",
     "iopub.status.busy": "2024-07-11T02:30:13.972480Z",
     "iopub.status.idle": "2024-07-11T02:30:14.081566Z",
     "shell.execute_reply": "2024-07-11T02:30:14.080581Z"
    },
    "id": "3-EPqtfHyP7Q",
    "outputId": "adf1acc3-d0f5-44a5-e460-3030a8938334",
    "papermill": {
     "duration": 0.132044,
     "end_time": "2024-07-11T02:30:14.084291",
     "exception": false,
     "start_time": "2024-07-11T02:30:13.952247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transfer = pd.read_csv('/kaggle/input/srbh2020-v2/dataset_capec_transfer (1).csv')\n",
    "df_transfer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04142617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.127379Z",
     "iopub.status.busy": "2024-07-11T02:30:14.126566Z",
     "iopub.status.idle": "2024-07-11T02:30:14.133242Z",
     "shell.execute_reply": "2024-07-11T02:30:14.132307Z"
    },
    "id": "OOGOmvXuyP7Q",
    "papermill": {
     "duration": 0.029798,
     "end_time": "2024-07-11T02:30:14.135480",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.105682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optional (not effect very much)\n",
    "# for word tokenizer instead of character tokenizer\n",
    "df_transfer['label'] = df_transfer['category']\n",
    "\n",
    "df_transfer = df_transfer[0:len(df_transfer)//training_parameters['batch_size']*training_parameters['batch_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd70745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.176389Z",
     "iopub.status.busy": "2024-07-11T02:30:14.176044Z",
     "iopub.status.idle": "2024-07-11T02:30:14.185659Z",
     "shell.execute_reply": "2024-07-11T02:30:14.184691Z"
    },
    "papermill": {
     "duration": 0.033218,
     "end_time": "2024-07-11T02:30:14.187881",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.154663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transfer['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ede591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.228685Z",
     "iopub.status.busy": "2024-07-11T02:30:14.228318Z",
     "iopub.status.idle": "2024-07-11T02:30:14.246301Z",
     "shell.execute_reply": "2024-07-11T02:30:14.245581Z"
    },
    "papermill": {
     "duration": 0.040647,
     "end_time": "2024-07-11T02:30:14.248393",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.207746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transfer = df_transfer[(df_transfer['label'] != '16 - Dictionary-based Password Attack') & (df_transfer['label'] != 'Normal')]\n",
    "df_transfer = df_transfer.sample(frac = 1)\n",
    "\n",
    "df_transfer = df_transfer[(df_transfer['label'] != 'Normal')]\n",
    "\n",
    "df_transfer = df_transfer[0:len(df_transfer)//training_parameters['batch_size']*training_parameters['batch_size']]\n",
    "df_train = df_train[0:len(df_train)//training_parameters['batch_size']*training_parameters['batch_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f36c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.287350Z",
     "iopub.status.busy": "2024-07-11T02:30:14.286760Z",
     "iopub.status.idle": "2024-07-11T02:30:14.424676Z",
     "shell.execute_reply": "2024-07-11T02:30:14.423217Z"
    },
    "papermill": {
     "duration": 0.160621,
     "end_time": "2024-07-11T02:30:14.427417",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.266796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dataset = ReviewDataset(df_train)\n",
    "source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06c1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.466573Z",
     "iopub.status.busy": "2024-07-11T02:30:14.466156Z",
     "iopub.status.idle": "2024-07-11T02:30:14.476020Z",
     "shell.execute_reply": "2024-07-11T02:30:14.475139Z"
    },
    "papermill": {
     "duration": 0.031839,
     "end_time": "2024-07-11T02:30:14.478220",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.446381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transfer['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e856fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.517714Z",
     "iopub.status.busy": "2024-07-11T02:30:14.517328Z",
     "iopub.status.idle": "2024-07-11T02:30:14.521502Z",
     "shell.execute_reply": "2024-07-11T02:30:14.520582Z"
    },
    "papermill": {
     "duration": 0.026279,
     "end_time": "2024-07-11T02:30:14.523576",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.497297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bfc996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.563651Z",
     "iopub.status.busy": "2024-07-11T02:30:14.563267Z",
     "iopub.status.idle": "2024-07-11T02:30:14.662252Z",
     "shell.execute_reply": "2024-07-11T02:30:14.661111Z"
    },
    "id": "A8B1VE0uyP7Q",
    "papermill": {
     "duration": 0.122295,
     "end_time": "2024-07-11T02:30:14.665033",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.542738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_dataset = ReviewDataset(df_transfer)\n",
    "target_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36358716",
   "metadata": {
    "id": "I0_eFxZhyP7R",
    "papermill": {
     "duration": 0.019509,
     "end_time": "2024-07-11T02:30:14.704705",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.685196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5b4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.746223Z",
     "iopub.status.busy": "2024-07-11T02:30:14.745840Z",
     "iopub.status.idle": "2024-07-11T02:30:14.757357Z",
     "shell.execute_reply": "2024-07-11T02:30:14.756508Z"
    },
    "id": "eZHfFVKGyP7R",
    "papermill": {
     "duration": 0.035082,
     "end_time": "2024-07-11T02:30:14.759536",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.724454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "\n",
    "        num_labels = config[\"num_labels\"]\n",
    "        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT') # model that we will use\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "        self.prj = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"]//2);\n",
    "        self.prj2 = nn.Linear(config[\"hidden_size\"]//2, config[\"hidden_size\"]//16);\n",
    "        self.attack_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"]//16, num_labels),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "#       Freeze bert layer\n",
    "        modules = [self.bert.embeddings, self.bert.encoder.layer[:2]] #Replace value by what you want\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          labels=None,\n",
    "#           grl_lambda = 1.0,\n",
    "          ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "\n",
    "#         pooled_output = outputs[1] # For bert-base-uncase\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        pooled_output_prj = self.prj(pooled_output)\n",
    "        pooled_output_prj2 = self.prj2(pooled_output_prj)\n",
    "        attack_pred = self.attack_classifier(pooled_output_prj2)\n",
    "\n",
    "        return attack_pred.to(device), pooled_output_prj, pooled_output_prj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfb8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.798861Z",
     "iopub.status.busy": "2024-07-11T02:30:14.798469Z",
     "iopub.status.idle": "2024-07-11T02:30:14.804779Z",
     "shell.execute_reply": "2024-07-11T02:30:14.803859Z"
    },
    "id": "la-EEL6jyP7R",
    "papermill": {
     "duration": 0.028601,
     "end_time": "2024-07-11T02:30:14.806850",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.778249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    predicted_labels_dict = {\n",
    "      0: 0,\n",
    "      1: 0,\n",
    "      2: 0,\n",
    "      3: 0,\n",
    "      4: 0,\n",
    "      5: 0,\n",
    "        6:0\n",
    "    }\n",
    "\n",
    "    predicted_label = logits.max(dim = 1)[1]\n",
    "\n",
    "    for pred in predicted_label:\n",
    "        # print(pred.item())\n",
    "        predicted_labels_dict[pred.item()] += 1\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "\n",
    "    return acc, predicted_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912321e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.846269Z",
     "iopub.status.busy": "2024-07-11T02:30:14.845391Z",
     "iopub.status.idle": "2024-07-11T02:30:14.861711Z",
     "shell.execute_reply": "2024-07-11T02:30:14.860684Z"
    },
    "id": "DRq0TLuayP7R",
    "papermill": {
     "duration": 0.038976,
     "end_time": "2024-07-11T02:30:14.863951",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.824975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "def evaluate(model, dataset = \"target\", percentage = 80):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        predicted_labels_dict = {\n",
    "          0: 0,\n",
    "          1: 0,\n",
    "          2: 0,\n",
    "          3: 0,\n",
    "          4: 0,\n",
    "          5: 0,\n",
    "        6:0\n",
    "        }\n",
    "        model.eval()\n",
    "        dev_df = pd.read_csv(\"/kaggle/input/srbh2020-v2/dataset_capec_\" + dataset + \" (1).csv\")\n",
    "        dev_df['label'] = dev_df['category']\n",
    "        dev_df = dev_df[(dev_df['label'] != '16 - Dictionary-based Password Attack') & (dev_df['label'] != 'Normal')]\n",
    "        dev_df = dev_df[(dev_df['label'] != 'Normal')]\n",
    "        data_size = dev_df.shape[0] \n",
    "        selected_for_evaluation = int(data_size*percentage/100)\n",
    "        dev_df = dev_df.head(selected_for_evaluation)\n",
    "        dataset = ReviewDataset(df_transfer)\n",
    "        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "        true_labels = list()\n",
    "        predicted_label = list()\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids.squeeze(axis=1),\n",
    "                \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "            attack_pred, _, _ = model(**inputs)\n",
    "            true_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            predicted_label.extend(attack_pred.max(dim = 1)[1].cpu().numpy())\n",
    "            _, predicted_labels = compute_accuracy(attack_pred, inputs[\"labels\"])\n",
    "\n",
    "            for i in range(7):\n",
    "                  predicted_labels_dict[i] += predicted_labels[i]\n",
    "\n",
    "        score = f1_score(true_labels,predicted_label,average=\"macro\")\n",
    "        precision = precision_score(true_labels, predicted_label,average=\"macro\")\n",
    "        recall = recall_score(true_labels, predicted_label,average=\"macro\")\n",
    "        report = classification_report(true_labels,predicted_label,digits=4)\n",
    "        acc= accuracy_score(true_labels, predicted_label)\n",
    "        #classifaction_report_csv(report,precision,recall,score,0)\n",
    "        print ('\\n clasification report:\\n', report)\n",
    "        print ('F1 score:', score)\n",
    "        print ('Recall:', recall)\n",
    "        print ('Precision:', precision)\n",
    "        print ('Acc:', acc)\n",
    "        print('Confusion Matrix: \\n',confusion_matrix(true_labels, predicted_label))\n",
    "        print(predicted_labels_dict)\n",
    "    print(\"Testing time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1723fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:14.903943Z",
     "iopub.status.busy": "2024-07-11T02:30:14.903637Z",
     "iopub.status.idle": "2024-07-11T02:30:14.919310Z",
     "shell.execute_reply": "2024-07-11T02:30:14.918321Z"
    },
    "papermill": {
     "duration": 0.037682,
     "end_time": "2024-07-11T02:30:14.921455",
     "exception": false,
     "start_time": "2024-07-11T02:30:14.883773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report,accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "def evaluate_v2(model, dataset = \"target\", percentage = 80):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        predicted_labels_dict = {\n",
    "          0: 0,\n",
    "          1: 0,\n",
    "          2: 0,\n",
    "          3: 0,\n",
    "          4: 0,\n",
    "          5: 0,\n",
    "        6:0\n",
    "        }\n",
    "        model.eval()\n",
    "        dev_df = pd.read_csv(\"/kaggle/input/srbh2020-v2/dataset_capec_\" + dataset + \" (1).csv\")\n",
    "        dev_df['label'] = dev_df['category']\n",
    "        dev_df = dev_df[(dev_df['label'] != '16 - Dictionary-based Password Attack') & (dev_df['label'] != 'Normal')]\n",
    "        dev_df = dev_df[(dev_df['label'] != 'Normal')]\n",
    "        data_size = dev_df.shape[0] \n",
    "        selected_for_evaluation = int(data_size*percentage/100)\n",
    "        dev_df = dev_df.head(selected_for_evaluation)\n",
    "        dataset = ReviewDataset(df_transfer[:3000])\n",
    "        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "        true_labels = list()\n",
    "        predicted_label = list()\n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids.squeeze(axis=1),\n",
    "                \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "            attack_pred, _, _ = model(**inputs)\n",
    "            true_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            predicted_label.extend(attack_pred.max(dim = 1)[1].cpu().numpy())\n",
    "            _, predicted_labels = compute_accuracy(attack_pred, inputs[\"labels\"])\n",
    "\n",
    "            for i in range(7):\n",
    "                  predicted_labels_dict[i] += predicted_labels[i]\n",
    "\n",
    "        score = f1_score(true_labels,predicted_label,average=\"macro\")\n",
    "        precision = precision_score(true_labels, predicted_label,average=\"macro\")\n",
    "        recall = recall_score(true_labels, predicted_label,average=\"macro\")\n",
    "        report = classification_report(true_labels,predicted_label,digits=4)\n",
    "        acc= accuracy_score(true_labels, predicted_label)\n",
    "        #classifaction_report_csv(report,precision,recall,score,0)\n",
    "        print ('\\n clasification report:\\n', report)\n",
    "        print ('F1 score:', score)\n",
    "        print ('Recall:', recall)\n",
    "        print ('Precision:', precision)\n",
    "        print ('Acc:', acc)\n",
    "        print('Confusion Matrix: \\n',confusion_matrix(true_labels, predicted_label))\n",
    "        print(predicted_labels_dict)\n",
    "    print(\"Testing time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed2877",
   "metadata": {
    "id": "PJWJi_UoyP7S",
    "papermill": {
     "duration": 0.019064,
     "end_time": "2024-07-11T02:30:15.477096",
     "exception": false,
     "start_time": "2024-07-11T02:30:15.458032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2ecf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:30:15.563668Z",
     "iopub.status.busy": "2024-07-11T02:30:15.563254Z",
     "iopub.status.idle": "2024-07-11T06:26:26.707494Z",
     "shell.execute_reply": "2024-07-11T06:26:26.705926Z"
    },
    "id": "B0SjEiJPyP7S",
    "outputId": "4676061d-47c7-4c03-e162-e105e5d32adf",
    "papermill": {
     "duration": 14171.191696,
     "end_time": "2024-07-11T06:26:26.733916",
     "exception": false,
     "start_time": "2024-07-11T02:30:15.542220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Function to initialize the model and optimizer\n",
    "def initialize_model_and_optimizer():\n",
    "    model = DomainAdaptationModel()  # Initialize the domain adaptation model\n",
    "    model.to(device)  # Move the model to the specified device (e.g., GPU)\n",
    "    optimizer = optim.Adam(model.parameters(), training_parameters[\"learning_rate\"])  # Use Adam optimizer\n",
    "    return model, optimizer\n",
    "\n",
    "# Function to initialize the MK-MMD loss\n",
    "def initialize_mkmmd_loss():\n",
    "    return MultipleKernelMaximumMeanDiscrepancy(\n",
    "        kernels=[GaussianKernel(alpha=2 ** k) for k in range(-3, 2)],  # Define Gaussian kernels\n",
    "        linear=True\n",
    "    )\n",
    "\n",
    "# Function to prepare inputs for the model\n",
    "def prepare_inputs(batch, device):\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0].squeeze(axis=1),  # Squeeze input IDs\n",
    "        \"attention_mask\": batch[1].squeeze(axis=1),  # Squeeze attention mask\n",
    "        \"token_type_ids\": batch[2].squeeze(axis=1),  # Squeeze token type IDs\n",
    "        \"labels\": batch[3],  # Labels\n",
    "    }\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.to(device)  # Move inputs to the specified device\n",
    "    return inputs\n",
    "\n",
    "# Function to train the model for one epoch\n",
    "def train_one_epoch(model, optimizer, mkmmd_loss, source_dataloader, target_dataloader, max_batches, epoch_idx):\n",
    "    source_iterator = iter(source_dataloader)  # Iterator for source dataloader\n",
    "    target_iterator = iter(target_dataloader)  # Iterator for target dataloader\n",
    "    mean_clf, mean_da_mmd, mean_total, mean_grl = 0., 0., 0., 0.  # Initialize metrics\n",
    "    plot_da_mmd, plot_clf, plot_total, plot_grl = [], [], [], []  # Initialize plots\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        # Compute gradient reversal lambda\n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n",
    "        grl_lambda = 0.5 * (2. / (1. + np.exp(-7 * p)) - 1)\n",
    "        mean_grl += grl_lambda\n",
    "        grl_lambda = torch.tensor(grl_lambda)\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        mkmmd_loss.train()  # Set MK-MMD loss to training mode\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Source dataset training update\n",
    "        source_batch = next(source_iterator)  # Get the next batch from source dataloader\n",
    "        source_inputs = prepare_inputs(source_batch, device)  # Prepare inputs\n",
    "        attack_pred, pooled_output_prj_source, _ = model(**source_inputs)  # Forward pass\n",
    "        loss_s_attack = torch.nn.NLLLoss()(attack_pred, source_inputs[\"labels\"])  # Compute classification loss\n",
    "\n",
    "        # Target dataset training update\n",
    "        target_batch = next(target_iterator)  # Get the next batch from target dataloader\n",
    "        target_inputs = prepare_inputs(target_batch, device)  # Prepare inputs\n",
    "        _, pooled_output_prj_target, _ = model(**target_inputs)  # Forward pass\n",
    "\n",
    "        # Compute transfer loss using MK-MMD\n",
    "        transfer_loss = mkmmd_loss(pooled_output_prj_source, pooled_output_prj_target)\n",
    "        loss = loss_s_attack + transfer_loss * grl_lambda  # Total loss\n",
    "\n",
    "        # Update metrics\n",
    "        mean_clf += loss_s_attack.item()\n",
    "        mean_da_mmd += transfer_loss.item()\n",
    "        mean_total += loss.item()\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log metrics every 200 batches\n",
    "        if batch_idx % 200 == 0 and batch_idx != 0:\n",
    "            plot_da_mmd.append(mean_da_mmd / 200)\n",
    "            plot_clf.append(mean_clf / 200)\n",
    "            plot_total.append(mean_total / 200)\n",
    "            plot_grl.append(mean_grl / 200)\n",
    "            mean_clf, mean_da_mmd, mean_total, mean_grl = 0., 0., 0., 0.\n",
    "\n",
    "    return plot_da_mmd, plot_clf, plot_total, plot_grl\n",
    "\n",
    "# Function to train the model\n",
    "def train_model():\n",
    "    model, optimizer = initialize_model_and_optimizer()  # Initialize model and optimizer\n",
    "    mkmmd_loss = initialize_mkmmd_loss()  # Initialize MK-MMD loss\n",
    "    max_batches = min(len(source_dataloader), len(target_dataloader))  # Determine the maximum number of batches\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    for epoch_idx in range(training_parameters[\"epochs\"]):  # Loop over epochs\n",
    "        # Train for one epoch\n",
    "        plot_da_mmd, plot_clf, plot_total, plot_grl = train_one_epoch(\n",
    "            model, optimizer, mkmmd_loss, source_dataloader, target_dataloader, max_batches, epoch_idx\n",
    "        )\n",
    "        print(f\"Epoch: {epoch_idx}\")  # Print epoch number\n",
    "        evaluate_v2(model, dataset=\"combine\", percentage=10)  # Evaluate the model\n",
    "\n",
    "    print(\"Training time:\", time.time() - start_time)  # Print total training time\n",
    "\n",
    "# Start training\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968691b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:26:26.784592Z",
     "iopub.status.busy": "2024-07-11T06:26:26.784147Z",
     "iopub.status.idle": "2024-07-11T06:26:27.420508Z",
     "shell.execute_reply": "2024-07-11T06:26:27.419416Z"
    },
    "id": "WASH2AuA6HOA",
    "papermill": {
     "duration": 0.664859,
     "end_time": "2024-07-11T06:26:27.422879",
     "exception": false,
     "start_time": "2024-07-11T06:26:26.758020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "\n",
    "def mean_blocks(data, block_size):\n",
    "    return [np.mean(data[i:i+block_size]) for i in range(0, len(data), block_size)]\n",
    "\n",
    "y1 = np.array(plot_da)\n",
    "y2 = np.array(plot_clf)\n",
    "y3 = np.array(plot_total)\n",
    "y4 = np.array(plot_grl)\n",
    "\n",
    "\n",
    "plt.plot(y4)\n",
    "plt.title(\"GRL\")\n",
    "plt.show()\n",
    "plt.plot(y2)\n",
    "plt.title(\"MMD\")\n",
    "plt.legend([\"transfer loss\",\"classification loss\", \"total_loss\", \"grl\"], loc =\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d4a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:26:27.477028Z",
     "iopub.status.busy": "2024-07-11T06:26:27.476094Z",
     "iopub.status.idle": "2024-07-11T06:26:28.085997Z",
     "shell.execute_reply": "2024-07-11T06:26:28.084927Z"
    },
    "id": "p-ltNCmb11bf",
    "papermill": {
     "duration": 0.639751,
     "end_time": "2024-07-11T06:26:28.088502",
     "exception": false,
     "start_time": "2024-07-11T06:26:27.448751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(n_epochs)  +  training_parameters[\"output_file\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0552a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:26:28.140918Z",
     "iopub.status.busy": "2024-07-11T06:26:28.140157Z",
     "iopub.status.idle": "2024-07-11T06:29:59.602762Z",
     "shell.execute_reply": "2024-07-11T06:29:59.601501Z"
    },
    "id": "N_LIGo0ByP7S",
    "outputId": "faabff84-fe94-4a72-83fa-ffa42c3cfa4b",
    "papermill": {
     "duration": 211.519428,
     "end_time": "2024-07-11T06:29:59.633235",
     "exception": false,
     "start_time": "2024-07-11T06:26:28.113807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate(model, dataset = \"transfer\", percentage = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecb9e0",
   "metadata": {
    "id": "EZ0p0i7oyP7S",
    "papermill": {
     "duration": 0.026063,
     "end_time": "2024-07-11T06:29:59.685436",
     "exception": false,
     "start_time": "2024-07-11T06:29:59.659373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get accuracy on source and target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83abfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:29:59.740496Z",
     "iopub.status.busy": "2024-07-11T06:29:59.740102Z",
     "iopub.status.idle": "2024-07-11T06:33:36.230634Z",
     "shell.execute_reply": "2024-07-11T06:33:36.229281Z"
    },
    "id": "ETfYv4hB_Oiw",
    "outputId": "2b295441-08c8-42f0-95f7-c1e7c46fbc54",
    "papermill": {
     "duration": 216.548658,
     "end_time": "2024-07-11T06:33:36.260407",
     "exception": false,
     "start_time": "2024-07-11T06:29:59.711749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(model, dataset = \"combine\", percentage = 10)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3149701,
     "sourceId": 5447421,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3532088,
     "sourceId": 6157298,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4470122,
     "sourceId": 7719583,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4560209,
     "sourceId": 7790648,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4578711,
     "sourceId": 7815837,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4594809,
     "sourceId": 7838360,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 10599,
     "sourceId": 12822,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 11263,
     "sourceId": 13610,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 11264,
     "sourceId": 13611,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 11368,
     "sourceId": 13739,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 13621,
     "sourceId": 16346,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 24285,
     "sourceId": 28845,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 24680,
     "sourceId": 29301,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 24686,
     "sourceId": 29307,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 55208,
     "sourceId": 66189,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 55522,
     "sourceId": 66597,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16165.690188,
   "end_time": "2024-07-11T06:58:59.334999",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-11T02:29:33.644811",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
